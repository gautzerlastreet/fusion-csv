# tools/serp_checker.py

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import streamlit as st
import streamlit.components.v1 as components

# --- USER-AGENT et en-tÃªtes pour Ã©viter le blocage et forcer le franÃ§ais ---
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/115.0.0.0 Safari/537.36"
)
HEADERS = {
    "User-Agent": USER_AGENT,
    "Accept-Language": "fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7"
}

@st.cache_data(ttl=3600, show_spinner=False)
def get_bing_results(query: str) -> list[dict]:
    """Scrape les rÃ©sultats Bing et conserve uniquement 10 URLs uniques par domaine."""
    url = "https://www.bing.com/search"
    params = {"q": query, "count": 20}
    resp = requests.get(url, headers=HEADERS, params=params, timeout=5)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    items = soup.select("li.b_algo")

    results = []
    seen_domains = set()
    for li in items:
        if len(results) >= 10:
            break
        h2 = li.find("h2")
        if not h2 or not h2.find("a"):
            continue
        link = h2.find("a")["href"]
        # Extraction du domaine principal
        domain = urlparse(link).netloc.lower()
        if domain.startswith("www."):
            domain = domain[4:]
        # Conserver une seule URL par domaine
        if domain in seen_domains:
            continue
        seen_domains.add(domain)

        title = h2.get_text()
        snippet = li.find("p").get_text() if li.find("p") else ""
        results.append({
            "title": title,
            "link": link,
            "snippet": snippet
        })

    return results


def run():
    """Point dâ€™entrÃ©e Streamlit pour le SERP Checker Bing (sans API ni Google)."""
    st.header("ğŸ…±ï¸ Comparateur SERP Bing (sans API ni Google)")
    query = st.text_input("Entrez un mot-clÃ©", placeholder="ex. â€œoptimisation SEO Pythonâ€")
    if st.button("Lancer la recherche") and query:
        with st.spinner("Scraping Bing en coursâ€¦"):
            bing_results = get_bing_results(query)

        if not bing_results:
            st.warning("Aucun rÃ©sultat trouvÃ© ou blocage du scraping Bing.")
            return

        st.subheader("ğŸ…±ï¸ Bing (10 premiers - domaines uniques)")
        for r in bing_results:
            st.markdown(f"**[{r['title']}]({r['link']})**  \n{r['snippet']}")

        # PrÃ©paration de la liste d'URLs
        urls = [r["link"] for r in bing_results]
        urls_text = "\n".join(urls)

        # Bouton de tÃ©lÃ©chargement
        st.download_button(
            label="ğŸ“¥ TÃ©lÃ©charger les URLs",
            data=urls_text,
            file_name="bing_urls.txt",
            mime="text/plain"
        )

        # Bouton de copie via components.html
        html = f"""
        <button id='copy-btn' style='padding:8px 12px; font-size:16px; margin-top:8px;'>ğŸ“‹ Copier les URLs</button>
        <script>
        const btn = document.getElementById('copy-btn');
        btn.addEventListener('click', () => {{
            const text = `{urls_text}`;
            navigator.clipboard.writeText(text).then(() => {{
                alert('âœ… URLs copiÃ©es dans le presse-papier !');
            }});
        }});
        </script>
        """
        components.html(html, height=75)
